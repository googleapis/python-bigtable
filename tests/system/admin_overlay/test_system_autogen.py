# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# This file is automatically generated by CrossSync. Do not edit manually.

from typing import Tuple
from google.cloud import bigtable_admin_v2 as admin_v2
from google.cloud.bigtable.data._cross_sync import CrossSync
from google.cloud.bigtable.data import mutations, read_rows_query
from google.cloud.environment_vars import BIGTABLE_EMULATOR
from .conftest import (
    INSTANCE_PREFIX,
    BACKUP_PREFIX,
    ROW_PREFIX,
    DEFAULT_CLUSTER_LOCATIONS,
    REPLICATION_CLUSTER_LOCATIONS,
    TEST_TABLE_NAME,
    TEST_BACKUP_TABLE_NAME,
    TEST_COLUMMN_FAMILY_NAME,
    TEST_COLUMN_NAME,
    NUM_ROWS,
    INITIAL_CELL_VALUE,
    NEW_CELL_VALUE,
    generate_unique_suffix,
)
from datetime import datetime, timedelta
import pytest
import os
from google.api_core import operation as api_core_operation

if os.getenv(BIGTABLE_EMULATOR):
    pytest.skip(
        allow_module_level=True,
        reason="Emulator support for admin client tests unsupported.",
    )


@pytest.fixture(scope="session")
def data_client(admin_overlay_project_id):
    with CrossSync._Sync_Impl.DataClient(project=admin_overlay_project_id) as client:
        yield client


@pytest.fixture(scope="session")
def table_admin_client(admin_overlay_project_id):
    with admin_v2.BigtableTableAdminClient(
        client_options={"quota_project_id": admin_overlay_project_id}
    ) as client:
        yield client


@pytest.fixture(scope="session")
def instance_admin_client(admin_overlay_project_id):
    with admin_v2.BigtableInstanceAdminClient(
        client_options={"quota_project_id": admin_overlay_project_id}
    ) as client:
        yield client


@pytest.fixture(scope="session")
def instances_to_delete(instance_admin_client):
    instances = []
    try:
        yield instances
    finally:
        for instance in instances:
            instance_admin_client.delete_instance(name=instance.name)


@pytest.fixture(scope="session")
def backups_to_delete(table_admin_client):
    backups = []
    try:
        yield backups
    finally:
        for backup in backups:
            table_admin_client.delete_backup(name=backup.name)


def create_instance(
    instance_admin_client,
    table_admin_client,
    data_client,
    project_id,
    instances_to_delete,
    storage_type=admin_v2.StorageType.HDD,
    cluster_locations=DEFAULT_CLUSTER_LOCATIONS,
) -> Tuple[admin_v2.Instance, admin_v2.Table]:
    """Creates a new Bigtable instance with the specified project_id, storage type, and cluster locations.

    After creating the Bigtable instance, it will create a test table and populate it with dummy data.
    This is not defined as a fixture because the different system tests need different kinds of instances.
    """
    clusters = {}
    instance_id = generate_unique_suffix(INSTANCE_PREFIX)
    for idx, location in enumerate(cluster_locations):
        clusters[location] = admin_v2.Cluster(
            name=instance_admin_client.cluster_path(
                project_id, instance_id, f"{instance_id}-{idx}"
            ),
            location=instance_admin_client.common_location_path(project_id, location),
            default_storage_type=storage_type,
        )
    create_instance_request = admin_v2.CreateInstanceRequest(
        parent=instance_admin_client.common_project_path(project_id),
        instance_id=instance_id,
        instance=admin_v2.Instance(display_name=instance_id[:30]),
        clusters=clusters,
    )
    operation = instance_admin_client.create_instance(create_instance_request)
    instance = operation.result()
    instances_to_delete.append(instance)
    create_table_request = admin_v2.CreateTableRequest(
        parent=instance_admin_client.instance_path(project_id, instance_id),
        table_id=TEST_TABLE_NAME,
        table=admin_v2.Table(
            column_families={TEST_COLUMMN_FAMILY_NAME: admin_v2.ColumnFamily()}
        ),
    )
    table = table_admin_client.create_table(create_table_request)
    populate_table(table_admin_client, data_client, instance, table, INITIAL_CELL_VALUE)
    return (instance, table)


def populate_table(table_admin_client, data_client, instance, table, cell_value):
    """Populates all the test cells in the given table with the given cell value.

    This is used to populate test data when creating an instance, and for testing the
    wait_for_consistency call."""
    data_client_table = data_client.get_table(
        table_admin_client.parse_instance_path(instance.name)["instance"],
        table_admin_client.parse_table_path(table.name)["table"],
    )
    row_mutation_entries = []
    for i in range(0, NUM_ROWS):
        row_mutation_entries.append(
            mutations.RowMutationEntry(
                row_key=f"{ROW_PREFIX}-{i}",
                mutations=[
                    mutations.SetCell(
                        family=TEST_COLUMMN_FAMILY_NAME,
                        qualifier=TEST_COLUMN_NAME,
                        new_value=cell_value,
                        timestamp_micros=-1,
                    )
                ],
            )
        )
    data_client_table.bulk_mutate_rows(row_mutation_entries)


def create_backup(
    instance_admin_client, table_admin_client, instance, table, backups_to_delete
) -> admin_v2.Backup:
    """Creates a backup of the given table under the given instance.

    This will be restored to a different instance later on, to test
    optimize_restored_table."""
    list_clusters_response = instance_admin_client.list_clusters(parent=instance.name)
    cluster_name = list_clusters_response.clusters[0].name
    backup_id = generate_unique_suffix(BACKUP_PREFIX)
    operation = table_admin_client.create_backup(
        admin_v2.CreateBackupRequest(
            parent=cluster_name,
            backup_id=backup_id,
            backup=admin_v2.Backup(
                name=f"{cluster_name}/backups/{backup_id}",
                source_table=table.name,
                expire_time=datetime.now() + timedelta(hours=7),
            ),
        )
    )
    backup = operation.result()
    backups_to_delete.append(backup)
    return backup


def assert_table_cell_value_equal_to(
    table_admin_client, data_client, instance, table, value
):
    """Asserts that all cells in the given table have the given value."""
    data_client_table = data_client.get_table(
        table_admin_client.parse_instance_path(instance.name)["instance"],
        table_admin_client.parse_table_path(table.name)["table"],
    )
    query = read_rows_query.ReadRowsQuery(limit=NUM_ROWS)
    for row in data_client_table.read_rows_stream(query):
        latest_cell = row[TEST_COLUMMN_FAMILY_NAME, TEST_COLUMN_NAME][0]
        assert latest_cell.value.decode("utf-8") == value


@pytest.mark.parametrize(
    "second_instance_storage_type,expect_optimize_operation",
    [(admin_v2.StorageType.HDD, False), (admin_v2.StorageType.SSD, True)],
)
def test_optimize_restored_table(
    admin_overlay_project_id,
    instance_admin_client,
    table_admin_client,
    data_client,
    instances_to_delete,
    backups_to_delete,
    second_instance_storage_type,
    expect_optimize_operation,
):
    instance_with_backup, table_to_backup = create_instance(
        instance_admin_client,
        table_admin_client,
        data_client,
        admin_overlay_project_id,
        instances_to_delete,
        admin_v2.StorageType.HDD,
    )
    instance_to_restore, _ = create_instance(
        instance_admin_client,
        table_admin_client,
        data_client,
        admin_overlay_project_id,
        instances_to_delete,
        second_instance_storage_type,
    )
    backup = create_backup(
        instance_admin_client,
        table_admin_client,
        instance_with_backup,
        table_to_backup,
        backups_to_delete,
    )
    restore_operation = table_admin_client.restore_table(
        admin_v2.RestoreTableRequest(
            parent=instance_to_restore.name,
            table_id=TEST_BACKUP_TABLE_NAME,
            backup=backup.name,
        )
    )
    assert isinstance(restore_operation, admin_v2.RestoreTableOperation)
    restored_table = restore_operation.result()
    optimize_operation = restore_operation.optimize_restored_table_operation()
    if expect_optimize_operation:
        assert isinstance(optimize_operation, api_core_operation.Operation)
        optimize_operation.result()
    else:
        assert optimize_operation is None
    assert (
        restored_table.name
        == f"{instance_to_restore.name}/tables/{TEST_BACKUP_TABLE_NAME}"
    )
    assert_table_cell_value_equal_to(
        table_admin_client,
        data_client,
        instance_to_restore,
        restored_table,
        INITIAL_CELL_VALUE,
    )


def test_wait_for_consistency(
    instance_admin_client,
    table_admin_client,
    data_client,
    instances_to_delete,
    admin_overlay_project_id,
):
    instance, table = create_instance(
        instance_admin_client,
        table_admin_client,
        data_client,
        admin_overlay_project_id,
        instances_to_delete,
        cluster_locations=REPLICATION_CLUSTER_LOCATIONS,
    )
    populate_table(table_admin_client, data_client, instance, table, NEW_CELL_VALUE)
    wait_for_consistency_request = admin_v2.WaitForConsistencyRequest(
        name=table.name, standard_read_remote_writes=admin_v2.StandardReadRemoteWrites()
    )
    table_admin_client.wait_for_consistency(wait_for_consistency_request)
    assert_table_cell_value_equal_to(
        table_admin_client, data_client, instance, table, NEW_CELL_VALUE
    )
